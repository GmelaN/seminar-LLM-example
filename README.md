## 2024 CS 세미나: "LLM 기반 서비스의 개발 동향" - LLM 예제 코드
- 학과 내 세미나 소모임 발표에서 시연한 Ollama, FastAPI 기반 LLM 서비스의 예제 코드입니다.

## 환경
- ubuntu 23.10 AMD64
- GTX 1650 4GB
- Intel i7 13th & 16GB RAM
- Ollama >= 0.1.28
- llama2:13b
- FastAPI 
